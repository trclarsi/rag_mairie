Sur la base des documents fournis, le terme ¬´ stemming ¬ª (ou racinisation en fran√ßais) n'est pas explicitement mentionn√© dans les sources.
Cependant, les sources d√©crivent en d√©tail plusieurs √©tapes de pr√©traitement et de structuration des donn√©es textuelles qui s'inscrivent dans la m√™me logique de pr√©paration du texte pour un syst√®me d'IA :
‚Ä¢ La Tokenisation : Les sources expliquent que les mod√®les de langage ne traitent pas le texte tel quel, mais le divisent en unit√©s plus petites appel√©es tokens. Un token peut √™tre un mot entier, une partie de mot ou un caract√®re. Cette √©tape permet de convertir le langage en repr√©sentations num√©riques.
‚Ä¢ Le D√©coupage (Chunking) : Cette √©tape consiste √† fragmenter les documents en unit√©s s√©mantiquement autonomes (chunks) pour √©viter de d√©passer la taille maximale de la fen√™tre de contexte du mod√®le.
‚Ä¢ Le Nettoyage des donn√©es : Mentionn√© comme une √©tape cl√© du traitement, il inclut la suppression des doublons, la correction d'erreurs typographiques et l'alignement des formats.
‚Ä¢ La Vectorisation (Embedding) : Au lieu de r√©duire les mots √† leur racine (comme le ferait le stemming), les sources privil√©gient l'utilisation d'embeddings. Ces vecteurs num√©riques capturent les relations s√©mantiques et le contexte global des mots, permettant par exemple de distinguer les diff√©rents sens du mot ¬´ souris ¬ª selon la phrase.
Information hors sources : Pour votre compr√©hension, le stemming est une technique classique de traitement du langage naturel (NLP) qui consiste √† r√©duire un mot √† sa racine (son "stem") en supprimant ses suffixes (par exemple, "marcher", "marchons", "march√©" deviennent "march-"). Bien que tr√®s utile dans les moteurs de recherche traditionnels, cette technique est moins centrale dans les syst√®mes RAG modernes bas√©s sur les embeddings contextuels (comme BERT ou GPT) pr√©sent√©s dans vos sources, car ces mod√®les pr√©f√®rent conserver le mot entier ou ses sous-unit√©s pour en saisir la nuance exacte dans une phrase.
Fais une synthese de toutes les sources




Les sources fournissent un guide complet pour concevoir, d√©ployer et optimiser un syst√®me de G√©n√©ration Augment√©e par R√©cup√©ration (RAG), en prenant pour exemple concret la cr√©ation d'un assistant virtuel pour une mairie. Cette approche permet de pallier les limites des mod√®les de langage (LLM) traditionnels, dont les connaissances sont fig√©es √† leur date d'entra√Ænement et qui peuvent √™tre sujets aux hallucinations.
Voici une synth√®se structur√©e de la mise en ≈ìuvre d'un tel syst√®me :
1. Gestion et Pr√©paration des Donn√©es
Le socle d'un bon RAG repose sur la qualit√© des donn√©es ing√©r√©es.
‚Ä¢ Extraction et Transformation : Il est n√©cessaire de transformer des donn√©es non structur√©es (PDF, scans d'actes, enregistrements audio) en texte structur√©. Des outils comme PyPDF2 pour les documents textuels, EasyOCR pour les images scann√©es, ou Whisper pour la transcription audio sont recommand√©s.
‚Ä¢ Cycle de vie des donn√©es : Une gestion rigoureuse comprend quatre √©tapes : l'acquisition, le stockage (SQL, NoSQL ou vectoriel), le traitement (nettoyage et enrichissement) et la gouvernance, notamment pour assurer la conformit√© au RGPD.
2. Le C≈ìur Technique : Indexation Vectorielle
Pour que l'IA puisse "rechercher" l'information, le texte doit √™tre converti en math√©matiques.
‚Ä¢ D√©coupage (Chunking) : Les documents longs sont fragment√©s en unit√©s plus petites (chunks) pour respecter les limites de tokens du LLM et maintenir la pr√©cision s√©mantique. Les strat√©gies incluent le d√©coupage r√©cursif avec chevauchement ou le d√©coupage bas√© sur la structure (Markdown/HTML).
‚Ä¢ Vectorisation (Embeddings) : Chaque segment est transform√© en vecteur num√©rique (repr√©sentation s√©mantique) par des mod√®les comme SBERT ou ceux de Mistral AI.
‚Ä¢ Base de Donn√©es Vectorielle : Les vecteurs sont stock√©s dans des outils comme Faiss, sp√©cialis√© dans la recherche de similarit√© rapide, permettant d'identifier les documents les plus proches d'une question utilisateur en millisecondes.
3. Orchestration et Mod√®le de Langage (LLM)
Une fois les donn√©es pr√™tes, le syst√®me doit g√©n√©rer une r√©ponse.
‚Ä¢ S√©lection du mod√®le : Le choix peut porter sur des LLM polyvalents (comme GPT-4 ou Mistral Large) ou des mod√®les plus petits et sp√©cialis√©s (SLM) qui sont moins gourmands en ressources et plus √©conomiques.
‚Ä¢ Ing√©nierie de Prompt : Le prompt syst√®me est crucial pour d√©finir l'identit√© de l'assistant, son objectif, ses sources autoris√©es et ses limites explicites (ex: ne pas inventer de proc√©dures).
‚Ä¢ Param√®tres techniques : Pour garantir des r√©ponses factuelles, il est conseill√© de r√©gler la temp√©rature √† 0.2, le Top-p √† 0.9, et de limiter les max_tokens √† 300.
4. Interface et Exp√©rience Utilisateur
Le d√©ploiement se fait souvent via une application interactive.
‚Ä¢ Streamlit est utilis√© pour cr√©er une interface de chat g√©rant l'historique de conversation via st.session_state, ce qui assure la coh√©rence des √©changes.
‚Ä¢ Classification d'intention : Un classifieur peut √™tre int√©gr√© pour d√©terminer si une question n√©cessite une recherche documentaire (intention "RAG") ou s'il s'agit d'une simple salutation (intention "CHAT").
5. √âvaluation et Am√©lioration Continue
Un syst√®me RAG ne s'arr√™te pas √† sa construction ; il doit √™tre mesur√© objectivement.
‚Ä¢ Framework Ragas : Il permet d'√©valuer la qualit√© du syst√®me via des m√©triques comme la fid√©lit√© (absence d'hallucinations), la pertinence de la r√©ponse, la pr√©cision du contexte et le rappel du contexte (capacit√© √† trouver l'information).
‚Ä¢ Boucle de Feedback : L'int√©gration de boutons üëç/üëé permet de collecter les retours des utilisateurs. Ces interactions sont stock√©es en base de donn√©es pour √™tre analys√©es via un tableau de bord, permettant des ajustements it√©ratifs du syst√®me.
Redonnes moi le contenu des sources tels quels est

Voici une pr√©sentation structur√©e et d√©taill√©e du contenu des sources fournies, organis√©e selon la progression logique du cours ¬´ D√©ploiement Pragmatique des Syst√®mes RAG et LLM ¬ª.
1. Introduction et Concept du LLM
Le cours propose une approche pragmatique pour concevoir un syst√®me RAG (Retrieval-Augmented Generation), appliqu√© ici √† la gestion des demandes citoyennes d'une mairie.
‚Ä¢ D√©finition d'un LLM : Un Large Language Model est un programme capable de comprendre et g√©n√©rer du texte humain en utilisant des milliards de param√®tres. Il fonctionne comme un ¬´ expert du langage ¬ª qui anticipe statistiquement le mot suivant.
‚Ä¢ Fonctionnement technique : Le texte est d√©compos√© en tokens (unit√©s linguistiques converties en nombres). La plupart des mod√®les modernes utilisent l'architecture Transformer (apparue en 2017), reposant sur un m√©canisme d'attention qui permet au mod√®le d'√©valuer simultan√©ment les relations entre tous les mots d'une phrase.
‚Ä¢ Limites : Les LLM peuvent souffrir d'hallucinations (inventer des faits), ont une fen√™tre de contexte limit√©e et leurs connaissances sont fig√©es √† la date de leur entra√Ænement.
2. Pr√©paration et Gestion des Donn√©es
Pour alimenter le RAG, il faut transformer des donn√©es non structur√©es en texte structur√©.
‚Ä¢ Extraction : Les sources mentionnent des outils comme PyPDF2, PyMuPDF ou Markitdown pour les PDF, EasyOCR ou GOT OCR pour les documents scann√©s, et Whisper pour la transcription audio.
‚Ä¢ Cycle de vie des donn√©es : Il comprend l'acquisition, le stockage (SQL pour le structur√©, vectoriel pour le s√©mantique), le traitement (nettoyage, enrichissement) et la gouvernance (conformit√© au RGPD).
3. Vectorisation et Indexation
La vectorisation (ou embedding) transforme le texte en vecteurs num√©riques capturant le sens s√©mantique.
‚Ä¢ Mod√®les d'embedding : Les mod√®les de phrases (BERT, SBERT, Mistral) sont pr√©f√©r√©s aux anciens mod√®les de mots (Word2Vec) car ils prennent en compte le contexte global (ex: distinguer le sens du mot ¬´ souris ¬ª).
‚Ä¢ D√©coupage (Chunking) : Puisque les LLM ne peuvent pas traiter des documents trop longs, on les fragmente en chunks. Les strat√©gies incluent le d√©coupage r√©cursif avec chevauchement (pour garder le contexte) ou le d√©coupage bas√© sur la structure (Markdown/HTML).
‚Ä¢ Bases de donn√©es vectorielles : Elles permettent des recherches rapides par similarit√©. Les sources citent FAISS (utilis√© dans le projet pour sa rapidit√©), Pinecone, Weaviate et Milvus.
4. Personnalisation et Prompt Syst√®me
Pour adapter le LLM √† la mairie, trois approches sont compar√©es : le Prompt Engineering (rapide mais limit√©), le Fine-Tuning (co√ªteux et rigide) et le RAG (flexible et pr√©cis).
‚Ä¢ Le Prompt Syst√®me : C'est l'instruction initiale d√©finissant le r√¥le, l'objectif et les limites de l'assistant. Un prompt efficace d√©finit des comportements obligatoires (ton courtois, citer ses sources) et interdits (ne pas traiter de donn√©es personnelles, ne pas inventer de proc√©dures).
‚Ä¢ Param√®tres techniques : Pour un usage administratif, on conseille une temp√©rature basse (0.2) pour la factualit√©, un Top-p de 0.9 et une limite de 300 tokens pour la concision.
5. Construction de l'Application (RAG & Streamlit)
L'interface est d√©velopp√©e avec Streamlit et orchestr√©e par des outils comme LangChain ou Llama Index.
‚Ä¢ M√©moire : L'historique des √©changes est g√©r√© via st.session_state pour assurer la coh√©rence de la conversation.
‚Ä¢ Flux RAG : La question de l'utilisateur est vectoris√©e, les segments les plus pertinents sont r√©cup√©r√©s dans l'index FAISS, puis r√©inject√©s dans le prompt envoy√© au LLM.
‚Ä¢ Classification d'intention : Un classifieur peut √™tre int√©gr√© pour d√©terminer si une question n√©cessite une recherche documentaire ("RAG") ou est une simple politesse ("CHAT").
6. √âvaluation et Feedback
‚Ä¢ Framework Ragas : Il mesure la performance du syst√®me via des m√©triques comme la fid√©lit√© (faithfulness), la pertinence de la r√©ponse, la pr√©cision du contexte et le rappel (context recall).
‚Ä¢ Boucle de feedback : L'ajout de boutons üëç/üëé permet de collecter l'avis des utilisateurs. Ces retours sont stock√©s dans une base SQL (via SQLAlchemy) et visualisables sur un tableau de bord.
Les sources concluent sur les perspectives d'avenir comme les Agents IA, les mod√®les √† long contexte et les nouveaux protocoles d'int√©gration comme le MCP.